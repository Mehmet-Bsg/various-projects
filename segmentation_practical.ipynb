{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehmet-Bsg/various-projects/blob/main/segmentation_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_YUqkutrdO-"
      },
      "source": [
        "# Segmentation\n",
        "\n",
        "In this notebook we will train and test a deep learning model to perform binray image segmentation. \n",
        "\n",
        "This notebook deals with Whole Slide Images (WSI). Whole slide imaging, also known as virtual microscopy, refers to scanning a complete microscope slide and creating a single high-resolution digital file. "
      ],
      "id": "y_YUqkutrdO-"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38cc29a8",
        "outputId": "1f67d355-09f3-4c52-c78b-6af9b07dec42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "import imageio\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "38cc29a8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBXjAU-KL1_3",
        "outputId": "c63af08c-22a3-4029-96d7-fc944fce52a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Data\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/gdrive/MyDrive/Data'"
      ],
      "id": "LBXjAU-KL1_3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc28ac0c"
      },
      "source": [
        "# Fetching data\n",
        "\n",
        "First step is to implement functions to send data to the training and testing pipeline. To do so, we implement the class DataGeneratorTissue used to select patches of images by customising keras DataGenerator class.  \n",
        "\n",
        "The function *get_random_patch_coord* randomly selects the coordinates of an image's patch, or crop. A *patch_coord* points to the top left corner of the corresponding patch. Thus the coordinates [y, x] gives for a patch of size [h, w] the image's patch *image* [y:y+h, x:x+w].\n",
        "\n",
        "The function *choose_patch_coord* calls *get_random_patch_coord* and should return the coordinates of a patch with at most 70% of background labels (pixel value of 0 in label). If it cannot be achieved after 10 tryouts, the function should return the coordinates of the patch with the lowest background labels found.  "
      ],
      "id": "fc28ac0c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1f786f4a"
      },
      "outputs": [],
      "source": [
        "class DataGeneratorTissue(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, path, prefix, batch_size, \n",
        "                 patch_size, nb_channels_in=3, nb_channels_out=1,\n",
        "                 geometric_augmentations=[], augmentation_ratio=None):\n",
        "        \n",
        "        self.path = path\n",
        "        self.prefix = prefix\n",
        "        self.get_data_dict()\n",
        "        self.batch_size = batch_size\n",
        "        self.patch_size = patch_size\n",
        "        self.nb_channels_in = nb_channels_in\n",
        "        self.nb_channels_out = nb_channels_out\n",
        "        self.geometric_augmentations = []\n",
        "        self.augmentation_ratio = augmentation_ratio\n",
        "        \n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, batch_idx):\n",
        "        batch_data_idxs = self.data_idxs[batch_idx*self.batch_size:(batch_idx+1)*self.batch_size]\n",
        "        x, y = self.fetch_data(batch_data_idxs)\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.data_idxs = np.arange(len(self.data))\n",
        "        np.random.shuffle(self.data_idxs)\n",
        "    \n",
        "    def get_data_dict(self):\n",
        "        self.data = {}\n",
        "        for data_idx, image_name in enumerate(os.listdir(os.path.join(*[self.path, self.prefix, \"jpg\"]))):\n",
        "            self.data[data_idx] = image_name\n",
        "    \n",
        "    def get_random_patch_coord(self, image_size):\n",
        "        '''\n",
        "        image_size: (height, width)\n",
        "        '''\n",
        "        \n",
        "        \n",
        "        inf_coord = [0,0]\n",
        "        sup_coord = [image_size[0]-self.patch_size[0], image_size[1]-self.patch_size[1]]\n",
        "        patch_coord = np.random.randint(inf_coord, sup_coord)\n",
        "\n",
        "        ####  \n",
        "        \n",
        "        return patch_coord \n",
        "    \n",
        "    def choose_patch_coord(self, image_shape, label, threshold=0.7):\n",
        "        '''\n",
        "        image_shape: (height, width)\n",
        "        label: shape (height, width, channels)\n",
        "        '''\n",
        "        best_patch_coord = None\n",
        "        lowest_bg_per = np.inf\n",
        "        \n",
        "\n",
        "        for _ in range(10):\n",
        "            patch_coord = self.get_random_patch_coord(image_shape)\n",
        "            label_patch = label[patch_coord[0] : patch_coord[0] + self.patch_size[0], patch_coord[1] : patch_coord[1] + self.patch_size[1],0]\n",
        "            nb_pixels_b = np.sum(np.where(label_patch[...,0] == 0,1,0))\n",
        "            b_per = nb_pixels_b / (label_patch.shape[0] * label_patch.shape[1])\n",
        "            \n",
        "            if b_per < threshold : \n",
        "              return patch_coord\n",
        "\n",
        "            if b_per < lowest_bg_per:\n",
        "              best_patch_coord = patch_coord\n",
        "              lowest_bg_per = b_per\n",
        "        \n",
        "            \n",
        "        return best_patch_coord\n",
        "         \n",
        "        \n",
        "    def fetch_data(self, batch_data_idxs):\n",
        "        x = np.empty([self.batch_size, *self.patch_size, self.nb_channels_in])\n",
        "        y = np.empty([self.batch_size, *self.patch_size, self.nb_channels_out])\n",
        "        \n",
        "        for idx, data_idx in enumerate(batch_data_idxs):\n",
        "            image_file_path = os.path.join(*[self.path, self.prefix, \"jpg\", self.data[data_idx]])\n",
        "            image = imageio.imread(image_file_path)\n",
        "            \n",
        "            label_file_path = os.path.join(*[self.path, self.prefix, \"lbl\", self.data[data_idx]])\n",
        "            label = imageio.imread(label_file_path)\n",
        "            \n",
        "            image = np.float32(image/255)\n",
        "            label = np.expand_dims(np.float32(label/255), axis=-1)\n",
        "            \n",
        "            patch_coord = self.choose_patch_coord(list(image.shape[:2]), label)\n",
        "            \n",
        "            image_patch = image[patch_coord[0]:patch_coord[0]+self.patch_size[0], \n",
        "                                patch_coord[1]:patch_coord[1]+self.patch_size[1],:]\n",
        "            label_patch = label[patch_coord[0]:patch_coord[0]+self.patch_size[0], \n",
        "                                patch_coord[1]:patch_coord[1]+self.patch_size[1],:]\n",
        "            \n",
        "            if len(self.geometric_augmentations) != 0:\n",
        "                if np.random.binomial(1, self.augmentation_ratio) == 1:\n",
        "                    augm_idx = random.randint(0,len(self.geometric_augmentations)-1)\n",
        "                    image_patch, label_patch = self.geometric_augmentations[augm_idx](image_patch, label_patch)\n",
        "                              \n",
        "            x[idx:idx+1,...] = image_patch\n",
        "            y[idx:idx+1,...] = label_patch\n",
        "            \n",
        "        return x, y"
      ],
      "id": "1f786f4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "973e10af"
      },
      "source": [
        "# U-Net model builder\n",
        "\n",
        "The deep learning model we will use is a U-Net model.\n",
        "\n",
        "We implement the encoder and decoder parts of the U-Net model with skip connections in the *build_unet_model* function using only *conv_block*, *up_sampler* and *keras.layers.Concatenate()* functions."
      ],
      "id": "973e10af"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fbea0d76"
      },
      "outputs": [],
      "source": [
        "def conv_block(filters, strides, last_activation=None):\n",
        "    if last_activation == None:\n",
        "        conv_block = keras.Sequential([keras.layers.Conv2D(filters=filters, kernel_size=3, \n",
        "                                                           strides=1, padding=\"same\", \n",
        "                                                           kernel_initializer = \"he_normal\"),\n",
        "                                       keras.layers.BatchNormalization(),\n",
        "                                       keras.layers.LeakyReLU(), \n",
        "                                       keras.layers.Conv2D(filters=filters, kernel_size=3, \n",
        "                                                           strides=strides, padding=\"same\", \n",
        "                                                           kernel_initializer = \"he_normal\"), \n",
        "                                       keras.layers.BatchNormalization(), \n",
        "                                       keras.layers.LeakyReLU()])\n",
        "    else:\n",
        "        conv_block = keras.Sequential([keras.layers.Conv2D(filters=filters, kernel_size=3, \n",
        "                                                           strides=strides, padding=\"same\", \n",
        "                                                           kernel_initializer = \"he_normal\"),\n",
        "                                       keras.layers.BatchNormalization(),\n",
        "                                       keras.layers.Activation(last_activation)])\n",
        "    return conv_block\n",
        "\n",
        "def up_sampler(filters):\n",
        "    up_sampler = keras.Sequential([keras.layers.Conv2DTranspose(filters=filters, kernel_size=2, \n",
        "                                                                strides=2, padding=\"valid\", \n",
        "                                                                kernel_initializer = \"he_normal\"), \n",
        "                                   keras.layers.BatchNormalization(), \n",
        "                                   keras.layers.LeakyReLU()])\n",
        "    return up_sampler\n",
        "    \n",
        "def build_unet_model(nb_channels_in, nb_channels_out, \n",
        "                     unet_filters, last_activation, \n",
        "                     image_size=[None, None]):\n",
        "    inputs = keras.Input([*image_size, nb_channels_in])\n",
        "    \n",
        "    skip = []\n",
        "\n",
        "    \n",
        "    # Encoder part\n",
        "    import copy \n",
        "    inp = inputs \n",
        "\n",
        "    for j in range (len(unet_filters)):\n",
        "      inp = conv_block(filters = unet_filters[j], strides = 1, last_activation = last_activation)(inp)\n",
        "      if j != len(unet_filters) -1 :\n",
        "        skip.insert(0,(inp))\n",
        "        inp = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(inp)\n",
        "  \n",
        "    # Decoder part\n",
        "    for x,y in enumerate(range(len(unet_filters)-2, -1, -1)):\n",
        "      inp = up_sampler(filters = unet_filters[y])(inp)\n",
        "      inp = keras.layers.concatenate([skip[x],inp], axis = -1)\n",
        "      inp = conv_block(filters = unet_filters[y], strides = 1, last_activation = last_activation)(inp)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    outputs = conv_block(nb_channels_out, 1, last_activation)(inp)\n",
        "\n",
        "    unet_model = keras.Model(inputs, outputs)\n",
        "    return unet_model"
      ],
      "id": "fbea0d76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dee0ea60"
      },
      "source": [
        "# Segmentation loss\n",
        "\n",
        "The loss used here is the Jaccard loss, also known as Intersection over Union. As we are working with binary images as outputs, we want to maximize the overlap area of ground truth and predicted masks.\n",
        "\n",
        "We implement the computation of the cardinal of the intersection and the union between *y_true* and *y_pred*, stored in the variables *intersection* and *union* for the jaccard index-based loss. The function *K.sum()* should be sufficient to do so."
      ],
      "id": "dee0ea60"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "10d6c8f7"
      },
      "outputs": [],
      "source": [
        "def jaccard_loss(y_true, y_pred, smooth=1.0):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis = -1)\n",
        "    union = K.sum(K.abs(y_true) + K.abs(y_pred), axis = -1) - intersection\n",
        "\n",
        "\n",
        "    \n",
        "    return 1 - (intersection + smooth) / (union + smooth)"
      ],
      "id": "10d6c8f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b79b93cd"
      },
      "source": [
        "# Data augmentation\n"
      ],
      "id": "b79b93cd"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bc15cdf5"
      },
      "outputs": [],
      "source": [
        "def flip_aug(image, label):\n",
        "    flipped_image = np.flip(image, axis=1)\n",
        "    flipped_label = np.flip(label, axis=1)\n",
        "    return flipped_image, flipped_label\n",
        "\n",
        "def rot_aug(image, label):\n",
        "    image_rotation = ndimage.rotate(image, angle=random.randint(low=-20, high=20))\n",
        "    label_rotation = ndimage.rotate(label, angle=random.randint(low=-20, high=20))\n",
        "    return image_rotation, label_rotation"
      ],
      "id": "bc15cdf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac71ebf"
      },
      "source": [
        "# Model's training"
      ],
      "id": "4ac71ebf"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd27f3a7",
        "outputId": "c3009146-2afd-4500-bc07-02391a246dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 128, 128, 8)  256         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 64, 64, 8)    0           ['sequential[0][0]']             \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 64, 64, 16)   1232        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)  0           ['sequential_1[0][0]']           \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 32, 32, 32)   4768        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)  0           ['sequential_2[0][0]']           \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 16, 16, 64)   18752       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 32, 32, 32)   8352        ['sequential_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 64)   0           ['sequential_2[0][0]',           \n",
            "                                                                  'sequential_4[0][0]']           \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)      (None, 32, 32, 32)   18592       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " sequential_6 (Sequential)      (None, 64, 64, 16)   2128        ['sequential_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 64, 64, 32)   0           ['sequential_1[0][0]',           \n",
            "                                                                  'sequential_6[0][0]']           \n",
            "                                                                                                  \n",
            " sequential_7 (Sequential)      (None, 64, 64, 16)   4688        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " sequential_8 (Sequential)      (None, 128, 128, 8)  552         ['sequential_7[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 128, 16  0           ['sequential[0][0]',             \n",
            "                                )                                 'sequential_8[0][0]']           \n",
            "                                                                                                  \n",
            " sequential_9 (Sequential)      (None, 128, 128, 8)  1192        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " sequential_10 (Sequential)     (None, 128, 128, 1)  77          ['sequential_9[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,589\n",
            "Trainable params: 60,123\n",
            "Non-trainable params: 466\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "path = \"./tissue\"\n",
        "\n",
        "models_dir = \"models\"\n",
        "if os.path.exists(models_dir) == False:\n",
        "    os.mkdir(models_dir)\n",
        "\n",
        "model_id = 1\n",
        "\n",
        "dt = datetime.now()\n",
        "timestamp = str(dt.hour) + ':' + str(dt.minute) + ':' + str(dt.second) + '-' + str(dt.day) + ':' + str(dt.month) + ':' + str(dt.year)\n",
        "model_name = \"modelID=\" + str(model_id) +  \"_timestamp=\" + timestamp\n",
        "\n",
        "model_dir = os.path.join(models_dir, model_name)\n",
        "if os.path.exists(model_dir) == False:\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "nb_channels_in = 3\n",
        "nb_channels_out = 1\n",
        "last_activation = \"sigmoid\"\n",
        "unet_filters = [8, 16, 32, 64]\n",
        "patch_size = [128, 128]\n",
        "batch_size = 5\n",
        "lr = 0.0001\n",
        "nb_epochs = 20\n",
        "\n",
        "model_parameters = {\"nb_channels_in\": nb_channels_in,\n",
        "                    \"nb_channels_out\": nb_channels_out,\n",
        "                    \"last_activation\": last_activation,\n",
        "                    \"unet_filters\": unet_filters,\n",
        "                    \"patch_size\": patch_size,\n",
        "                    \"batch_size\": batch_size,\n",
        "                    \"lr\": lr,\n",
        "                    \"nb_epochs\": nb_epochs}\n",
        "\n",
        "json.dump(model_parameters, open(os.path.join(model_dir, \"model_parameters.json\"), \"w\"))\n",
        "\n",
        "model = build_unet_model(3, 1, unet_filters, \"sigmoid\", image_size = patch_size)\n",
        "model.summary()\n",
        "\n",
        "train_datagen = DataGeneratorTissue(path, \"train\", batch_size, \n",
        "                                    patch_size, 3, 1,\n",
        "                                    geometric_augmentations=[flip_aug, rot_aug], augmentation_ratio=0.2)\n",
        "\n",
        "val_datagen = DataGeneratorTissue(path, \"val\", batch_size, \n",
        "                                  patch_size, 3, 1)\n",
        "\n",
        "opt = keras.optimizers.Adam(lr)\n",
        "model.compile(optimizer=opt, loss=jaccard_loss)\n",
        "\n",
        "model_log_file_path = os.path.join(model_dir, \"log.csv\")\n",
        "best_model_weights_file_path = os.path.join(model_dir, \"best_model_weights.h5\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.CSVLogger(model_log_file_path),\n",
        "    keras.callbacks.ModelCheckpoint(best_model_weights_file_path, \n",
        "                                    save_best_only=True, save_weights_only=True)]\n"
      ],
      "id": "dd27f3a7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDcgRxyrGiBY",
        "outputId": "7ec4517b-eec5-46bc-c875-8fcbc19cdab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 138s 7s/step - loss: 0.3168 - val_loss: 0.3311\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 14s 735ms/step - loss: 0.3076 - val_loss: 0.3254\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 10s 496ms/step - loss: 0.2991 - val_loss: 0.3186\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 8s 380ms/step - loss: 0.2951 - val_loss: 0.3146\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 10s 482ms/step - loss: 0.2895 - val_loss: 0.3102\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 10s 513ms/step - loss: 0.2823 - val_loss: 0.3051\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 7s 373ms/step - loss: 0.2712 - val_loss: 0.3019\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 11s 519ms/step - loss: 0.2692 - val_loss: 0.2953\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 10s 511ms/step - loss: 0.2689 - val_loss: 0.2869\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2688 - val_loss: 0.2829\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 8s 377ms/step - loss: 0.2676 - val_loss: 0.2821\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 9s 461ms/step - loss: 0.2688 - val_loss: 0.2725\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 10s 512ms/step - loss: 0.2688 - val_loss: 0.2707\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 8s 379ms/step - loss: 0.2635 - val_loss: 0.2487\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 9s 476ms/step - loss: 0.2729 - val_loss: 0.2496\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 9s 458ms/step - loss: 0.2566 - val_loss: 0.2549\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 8s 402ms/step - loss: 0.2601 - val_loss: 0.2230\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 10s 505ms/step - loss: 0.2653 - val_loss: 0.2208\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 9s 477ms/step - loss: 0.2612 - val_loss: 0.2166\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 7s 368ms/step - loss: 0.2658 - val_loss: 0.2277\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5003ef7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model.fit(train_datagen, \n",
        "          epochs=nb_epochs,\n",
        "          validation_data=val_datagen, \n",
        "          verbose=1,\n",
        "          callbacks=callbacks)"
      ],
      "id": "BDcgRxyrGiBY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e66fbfb"
      },
      "source": [
        "# Training and Validation loss"
      ],
      "id": "0e66fbfb"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "42069cb3",
        "outputId": "f7cfe99d-086c-4ba8-afd8-ccb14f3de2d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa4f9f9d520>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.read_csv(model_log_file_path, sep=\",\")\n",
        "plt.plot(df[\"epoch\"], df[\"loss\"], \"r\", label=\"training loss\")\n",
        "plt.plot(df[\"epoch\"], df[\"val_loss\"], \"b\", label=\"validation loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()"
      ],
      "id": "42069cb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEn33k9V6-GX"
      },
      "source": [
        "### Some remarks : \n",
        "\n",
        "It seems like the traning and validation loss are not really similar. What is curious is that the loss don't follow the same varations. i was expecting two decreasing functions. It's either an error in my code or a curious distribution in the data between the training and validation set."
      ],
      "id": "tEn33k9V6-GX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ca5c9b9"
      },
      "source": [
        "# Model's inference function\n",
        "\n",
        "*make_apply_unet_model* is the function to call to embeds a U-Net model into a function in charge of the whole pipeline to get from an 8-bit image an 8-bit binary segmentation mask.  \n",
        "We implement the *apply_unet_model* function.\n",
        "The pipeline include:  \n",
        "* a [0,1]-normalization  \n",
        "* a padding of the image (so that its dimension are 2^{unet_depth = len(unet_filters)-1} divisible)  \n",
        "* the application of the UNet model  \n",
        "* an unpadding of the UNet model's output (to retrieve the initial image dimensions)  \n",
        "* the conversion of the UNet model's output into a binary segmentation mask by taking the label with the highest probability for each pixel  \n",
        "\n"
      ],
      "id": "6ca5c9b9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "914b9cb9"
      },
      "outputs": [],
      "source": [
        "def pad_image(image, unet_depth):\n",
        "    shape = image.shape[:2]  \n",
        "    if shape[0]%2**unet_depth != 0 or shape[1]%2**unet_depth != 0:\n",
        "        new_shape = [shape[0] + 2**unet_depth - shape[0]%2**unet_depth, \n",
        "                     shape[1] + 2**unet_depth - shape[1]%2**unet_depth]\n",
        "        new_image = np.empty([*new_shape, 3])\n",
        "        new_image[0:shape[0], 0:shape[1], :] = image[...]\n",
        "        \n",
        "        new_image[0:shape[0],shape[1]:new_shape[1],:] = image[:,shape[1]-(new_shape[1]-shape[1]):shape[1],:]\n",
        "        new_image[shape[0]:new_shape[0],0:shape[1],:] = image[shape[0]-(new_shape[0]-shape[0]):shape[0],:,:]\n",
        "        new_image[shape[0]:new_shape[0],shape[1]:new_shape[1],:] = image[shape[0]-(new_shape[0]-shape[0]):shape[0],shape[1]-(new_shape[1]-shape[1]):shape[1],:]\n",
        "        \n",
        "        return new_image\n",
        "    else:\n",
        "        return image\n",
        "    \n",
        "def unpad_image(image, old_shape):\n",
        "    im_shape = image.shape[:2]\n",
        "    if im_shape[0] == old_shape[0] and im_shape[1] == old_shape[1]:\n",
        "        return image\n",
        "    else:\n",
        "        new_image = image[0:old_shape[0], 0:old_shape[1], :]\n",
        "        return new_image\n",
        "    \n",
        "def apply_unet_model(unet_model, unet_depth, image):\n",
        "\n",
        "    image = np.array(image, dtype = float)/255 # normalization\n",
        "    image_shape = image.shape\n",
        "    second_image = pad_image(image, unet_depth) # padding the image into a new one named second image\n",
        "    second_image = np.expand_dims(second_image, axis = 0)\n",
        "\n",
        "    output = unet_model(second_image) # applying unet model\n",
        "    \n",
        "    output = np.where(output > 0.5, 1, 0)\n",
        "\n",
        "    ini_dim_image = unpad_image(output[0], image_shape) # unpadding the image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    return segm\n",
        "\n",
        "def make_apply_unet_model(unet_model, unet_depth):\n",
        "    return lambda image: apply_unet_model(unet_model, unet_depth, image)"
      ],
      "id": "914b9cb9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abaacc51"
      },
      "source": [
        "# Model's application on test dataset\n",
        "\n"
      ],
      "id": "abaacc51"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w4oBeUGHn1Tn"
      },
      "outputs": [],
      "source": [
        "indexes_to_test = [8, 42]  # indexes in [0, 42]"
      ],
      "id": "w4oBeUGHn1Tn"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oHDwnmhFDE5m"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "model2load_id = 1\n",
        "\n",
        "for dir in os.listdir(\"models\"):\n",
        "    if len(dir.split(\"modelID\")) > 1:\n",
        "        id = int(dir.split(\"_\")[0].split(\"=\")[1])\n",
        "        if id == model2load_id:\n",
        "            model_dir = os.path.join(\"models\", dir)\n",
        "            break\n",
        "\n",
        "with open(os.path.join(model_dir, \"model_parameters.json\"), \"r\") as model_parameters_file:\n",
        "    model_parameters = json.load(model_parameters_file)\n",
        "\n",
        "weights_path = os.path.join(model_dir, \"best_model_weights.h5\")\n",
        "\n",
        "model2 = build_unet_model(model_parameters[\"nb_channels_in\"], \n",
        "                          model_parameters[\"nb_channels_out\"], \n",
        "                          model_parameters[\"unet_filters\"], \n",
        "                          model_parameters[\"last_activation\"])\n",
        "\n",
        "model2.load_weights(weights_path)\n",
        "    \n",
        "apply_model =  make_apply_unet_model(model2, len(model_parameters[\"unet_filters\"])-1) \n",
        "\n",
        "test_images_dir = os.path.join(path, \"test\")\n",
        "test_images_filenames = os.listdir(os.path.join(test_images_dir, \"jpg\"))\n",
        "\n",
        "for test_image_idx in indexes_to_test:\n",
        "\n",
        "  image = imageio.imread(os.path.join(*[test_images_dir, \"jpg\", test_images_filenames[test_image_idx]]))\n",
        "  label = imageio.imread(os.path.join(*[test_images_dir, \"lbl\", test_images_filenames[test_image_idx]]))\n",
        "\n",
        "  segm = apply_model(image)\n",
        "\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(18, 18))\n",
        "\n",
        "  ax[0].imshow(image)\n",
        "  ax[0].set_title(\"Input\")\n",
        "  ax[1].imshow(segm, cmap=\"gray\")\n",
        "  ax[1].set_title(\"Prediction\")\n",
        "  ax[2].imshow(label, cmap=\"gray\")\n",
        "  ax[2].set_title(\"Target\")"
      ],
      "id": "oHDwnmhFDE5m"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}